{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the thread used by numpy \n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"  \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.interpolate as interp \n",
    "try:\n",
    "    import cupy as xp\n",
    "    import cupyx.scipy.interpolate as xinterp\n",
    "    # print(\"has cupy\")\n",
    "except (ImportError, ModuleNotFoundError) as e:\n",
    "    import numpy as xp\n",
    "    import scipy.interpolate as xinterp  \n",
    "    # print(\"no cupy\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "# matplotlib.rcParams['text.usetex'] = True\n",
    "# matplotlib.rcParams['font.family'] = 'serif'\n",
    "\n",
    "from Triangle.Constants import *\n",
    "from Triangle.Orbit import * \n",
    "from Triangle.Noise import *\n",
    "from Triangle.FFTTools import *\n",
    "from Triangle.TDI import *\n",
    "from Triangle.Data import * \n",
    "\n",
    "from Triangle_BBH.Waveform import * \n",
    "from Triangle_BBH.Response import *\n",
    "from Triangle_BBH.Utils import *\n",
    "from Triangle_BBH.Fisher import *\n",
    "\n",
    "np.random.seed(114514)\n",
    "xp.random.seed(114514)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5filename = \"/media/ps/One Touch/TDCII_Data/0_Verification_Dataset/data/VMBHB.h5\"\n",
    "with h5py.File(h5filename, \"r\") as h5file: \n",
    "    read_dict = read_dict_from_h5(h5file[\"/\"])\n",
    "read_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDI combination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORBIT_DIR = \"/home/ps/workspace/Triangle-Simulator/OrbitData/MicroSateOrbitEclipticTCB\"\n",
    "orbit = Orbit(OrbitDir=ORBIT_DIR)\n",
    "\n",
    "ltts = dict() \n",
    "for key in MOSA_labels: \n",
    "    ltts[key] = orbit.LTTfunctions()[key](read_dict[\"time\"])\n",
    "ltts = MOSADict(ltts)\n",
    "\n",
    "m = {\"eta\" : read_dict[\"eta\"]}\n",
    "fsample = 1. / (read_dict[\"time\"][1] - read_dict[\"time\"][0])\n",
    "\n",
    "tdi = TDI(measurements=m, delays=ltts, fsample=fsample, order=31) \n",
    "tdi.FastMichelson(channel=\"AET\")\n",
    "\n",
    "channel_names = [\"A2\", \"E2\", \"T2\"]\n",
    "for nch in channel_names: \n",
    "    tdi.measurements[nch][:1500] = 0. \n",
    "    tdi.measurements[nch][-1500:] = 0. \n",
    "\n",
    "data_time = read_dict[\"time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nch in channel_names: \n",
    "    tdi.measurements[nch] = downsampling(data=tdi.measurements[nch], fsample=fsample, downsample=5, kaiser_filter_coef=[240, 0.04, 0.16])\n",
    "    tdi.measurements[nch][:400] = 0. \n",
    "    tdi.measurements[nch][-400:] = 0. \n",
    "data_time = read_dict[\"time\"][::5]\n",
    "fsample = 1. / (data_time[1] - data_time[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nch in channel_names: \n",
    "    plt.plot(data_time/DAY, tdi.measurements[nch], linewidth=1, label=nch)\n",
    "plt.xlabel(\"Time (day)\")\n",
    "plt.ylabel(\"TDI\")\n",
    "plt.legend(loc = \"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = [] \n",
    "for ich, nch, cch in zip(range(3), channel_names, [BLUE, ORANGE, RED]):\n",
    "    ff, xf = FFT_window(tdi.measurements[nch], fsample, window_type=\"tukey\", window_args_dict=dict(alpha=0.05))\n",
    "    plt.loglog(ff, np.abs(xf), linewidth=1, color=cch, label=\"TDI-\"+nch)\n",
    "    data_channels.append(xf)\n",
    "data_frequency = ff \n",
    "data_channels = np.array(data_channels)\n",
    "\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Fourier transform of data (1/Hz)\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(linestyle=\":\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming an initial search step, we limit the frequency band to below 1.5e-2 Hz and above 1e-4 Hz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freq_idx = np.where((data_frequency<1.5e-2)&(data_frequency>1e-4))[0]\n",
    "\n",
    "data_frequency = xp.array(data_frequency[freq_idx])\n",
    "delta_f = data_frequency[1] - data_frequency[0]\n",
    "Tobs = 1. / delta_f\n",
    "\n",
    "data_channels = xp.array(data_channels[:, freq_idx])\n",
    "\n",
    "fiducial_parameters = read_dict[\"parameters\"]\n",
    "\n",
    "data_frequency.shape, data_channels.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### waveform and response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"primary\"\n",
    "use_gpu = True \n",
    "\n",
    "# initialize  waveform generator \n",
    "WFG = BBHxWaveformGenerator(mode=mode, use_gpu=use_gpu)\n",
    "\n",
    "# initialize response generator \n",
    "FDTDI = BBHxFDTDIResponseGenerator(orbit_class=orbit, waveform_generator=WFG, use_gpu=use_gpu)\n",
    "\n",
    "# response settings \n",
    "response_kwargs = dict(\n",
    "    modes=[(2,2),], \n",
    "    tmin=data_time[0]/DAY,\n",
    "    tmax=data_time[-1]/DAY, \n",
    "    tc_at_constellation=False, \n",
    "    TDIGeneration=\"2nd\", # \"1st\" or \"2nd\"\n",
    "    optimal_combination=True, # True for AET, False for XYZ \n",
    "    output_by_mode=False, \n",
    "    interpolation=True ,\n",
    ")\n",
    "\n",
    "response_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model noise \n",
    "- Notice that AET channels are only approximately independent. \n",
    "- The noise level of T channel varies significantly with time, thus we do not use T channel in subsequent analysis. \n",
    "- During realistic search we usually do not priorly know the PSD of noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise PSD model \n",
    "PSDfunc = TDIPSDs()\n",
    "\n",
    "# use the median time to calculate armlengths, this is only an approximation \n",
    "arm_time = (data_time[0] + data_time[-1]) / 2. \n",
    "arms = dict()\n",
    "for key in MOSA_labels:\n",
    "    arms[key] = orbit.LTTfunctions()[key](arm_time)\n",
    "print(\"arm time (day):\", arm_time / DAY)\n",
    "print(\"arm lengths:\", arms)\n",
    "\n",
    "# T will not be used \n",
    "PSD_channels = xp.array([\n",
    "    PSDfunc.PSD_A2_unequal(data_frequency.get(), arms), \n",
    "    PSDfunc.PSD_E2_unequal(data_frequency.get(), arms), \n",
    "    PSDfunc.PSD_T2_unequal(data_frequency.get(), arms)\n",
    "])\n",
    "\n",
    "# covariance matrix, this is also an approximate since the channels should be correlated under the unequal-arm regime\n",
    "CovMat = xp.array([\n",
    "    [PSD_channels[0], xp.zeros_like(PSD_channels[0]), xp.zeros_like(PSD_channels[0])],\n",
    "    [xp.zeros_like(PSD_channels[0]), PSD_channels[1], xp.zeros_like(PSD_channels[0])],\n",
    "    [xp.zeros_like(PSD_channels[0]), xp.zeros_like(PSD_channels[0]), PSD_channels[2]]\n",
    "]) / 4. / delta_f # (3, 3, Nf)\n",
    "\n",
    "# inverse of covmatrix\n",
    "InvCovMat = xp.linalg.inv(xp.transpose(CovMat, (2, 0, 1))) # (Nf, 3, 3)\n",
    "\n",
    "PSD_channels.shape, InvCovMat.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model with the true parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_channels = FDTDI.Response(parameters=fiducial_parameters, freqs=data_frequency, **response_kwargs)\n",
    "\n",
    "for ich, nch, cch in zip(range(3), channel_names, [BLUE, ORANGE, RED]):\n",
    "    plt.loglog(data_frequency.get(), np.abs(data_channels[ich].get()) * data_frequency.get() * 2., label=nch, linewidth=1, color=cch, alpha=0.5)\n",
    "    plt.loglog(data_frequency.get(), np.abs(model_channels[ich].get()) * data_frequency.get() * 2., linewidth=2, linestyle=\":\", color=cch)\n",
    "    plt.loglog(data_frequency.get(), np.sqrt(PSD_channels[ich] * data_frequency).get(), color=\"grey\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Characteristic Strain\")\n",
    "plt.grid(linestyle=\":\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use A, E channels only \n",
    "set $C^{-1}_{T_2 T_2} \\equiv 0$ to eliminate the contribution of $T_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InvCovMat[:, 2, 2] *= 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Like = Likelihood(\n",
    "    response_generator=FDTDI, \n",
    "    frequency=data_frequency, \n",
    "    data=data_channels, \n",
    "    invserse_covariance_matrix=InvCovMat, \n",
    "    response_parameters=response_kwargs, \n",
    "    use_gpu=use_gpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiducial_param_arr = ParamDict2ParamArr(fiducial_parameters)\n",
    "\n",
    "N_test=101\n",
    "test_idx = 4 # tc \n",
    "test_params = np.linspace(-1e-3, 1e-3, N_test) + fiducial_param_arr[test_idx]\n",
    "\n",
    "tmp_params = np.zeros((11, N_test))\n",
    "for i_test in range(N_test):\n",
    "    tmp_params[:, i_test] = fiducial_param_arr.copy()\n",
    "    tmp_params[test_idx][i_test] = test_params[i_test]\n",
    "test_lls_vec = Like.full_log_like_vectorized(tmp_params)\n",
    "plt.plot(test_params, test_lls_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MCMC Sampling \n",
    "\n",
    "the aim is to compare the posteriors of full and heterodyned likelihoods, so we choose ideal priors and starting points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eryn.ensemble import EnsembleSampler\n",
    "from eryn.state import State\n",
    "from eryn.prior import ProbDistContainer, uniform_dist\n",
    "from eryn.utils import TransformContainer\n",
    "from eryn.moves import GaussianMove, StretchMove, CombineMove\n",
    "from eryn.utils.utility import groups_from_inds\n",
    "from eryn.backends import HDFBackend\n",
    "from eryn.utils import SearchConvergeStopping\n",
    "\n",
    "from corner import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [r\"${\\rm lg}\\mathcal{M}_{c,z}$\", r\"$q$\", r\"$\\chi_{z,1}$\", r\"$\\chi_{z,2}$\", r\"$t_c$\", r\"$\\varphi_c$\", r\"${\\rm lg} D_L$\", r\"$\\cos \\iota$\", r\"$\\lambda$\", r\"$\\sin \\beta$\", r\"$\\psi$\"]\n",
    "\n",
    "truths = ParamDict2ParamArr(fiducial_parameters)\n",
    "\n",
    "truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper parameters\n",
    "ndim = 11 # dimension of paramters \n",
    "nwalkers = 100 # number of random walkers, limited by the vRAM of my 4080S, use fewer (e.g. 100) to speed up and more (e.g. 400) to get more smooth posterior \n",
    "ntemps = 10 # number of temperatures used in parallel tempering \n",
    "temps = np.array(list(np.power(2., np.arange(ntemps - 1))) + [np.infty]) \n",
    "betas = 1. / temps \n",
    "tempering_kwargs=dict(betas=betas)\n",
    "\n",
    "mcmc_moves = StretchMove(a=2) # mcmc move \n",
    "\n",
    "stop = None \n",
    "\n",
    "# set priors \n",
    "lim_lgMc = [5.5, 6.5]\n",
    "lim_q = [0.1, 0.999]\n",
    "lim_chiz1 = [-0.99, 0.99]\n",
    "lim_chiz2 = [-0.99, 0.99]\n",
    "lim_tc = [fiducial_parameters[\"coalescence_time\"] - 500/DAY, fiducial_parameters[\"coalescence_time\"] + 500/DAY] \n",
    "lim_phic = [0, TWOPI]\n",
    "lim_lgD = [4., 5.]\n",
    "lim_cosinc = [-1, 1]\n",
    "lim_lam = [0, TWOPI]\n",
    "lim_sinbeta = [-1, 1]\n",
    "lim_psi = [0, PI]\n",
    "\n",
    "lims = np.array([lim_lgMc, lim_q, lim_chiz1, lim_chiz2, lim_tc, lim_phic, lim_lgD, lim_cosinc, lim_lam, lim_sinbeta, lim_psi])\n",
    "lower_lims = lims[:, 0]\n",
    "upper_lims = lims[:, 1]\n",
    "\n",
    "priors_in = {i: uniform_dist(lims[i][0], lims[i][1]) for i in range(ndim)}\n",
    "priors = ProbDistContainer(priors_in)\n",
    "priors.use_cupy = False\n",
    "\n",
    "# set starting range \n",
    "start_lims = lims \n",
    "start_priors_in = {i: uniform_dist(start_lims[i][0], start_lims[i][1]) for i in range(ndim)}\n",
    "start_priors = ProbDistContainer(start_priors_in)\n",
    "start_priors.use_cupy = False\n",
    "\n",
    "lims, start_lims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eryn_like(params): \n",
    "    \"\"\"params: numpy array of shape (Nevents, Nparams)\"\"\"\n",
    "    return Like.full_log_like_vectorized(np.transpose(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ensemble = EnsembleSampler(\n",
    "    nwalkers,\n",
    "    ndim,\n",
    "    eryn_like, \n",
    "    priors,\n",
    "    args=[],\n",
    "    tempering_kwargs=tempering_kwargs,\n",
    "    stopping_fn=stop,\n",
    "    stopping_iterations=10,\n",
    "    moves=mcmc_moves,\n",
    "    vectorize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize starting positions throughout prior\n",
    "coords = start_priors.rvs(size=(ntemps, nwalkers,))\n",
    "print(coords.shape)\n",
    "\n",
    "thin_by = 100 \n",
    "burn = 0\n",
    "nsteps = int(100000 / thin_by) # should be more than enough \n",
    "\n",
    "ensemble.stopping_fn = None\n",
    "\n",
    "out = ensemble.run_mcmc(coords, nsteps, burn=burn, progress=True, thin_by=thin_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # resume run after the run is stopped \n",
    "# thin_by =100\n",
    "# burn = 0\n",
    "# nsteps = int(150000 / thin_by) # this should be far more than enough \n",
    "\n",
    "# out = ensemble.run_mcmc(ensemble.get_last_sample(), nsteps, burn=burn, progress=True, thin_by=thin_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show moves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thin = 10\n",
    "burnin = 0\n",
    "len_chain = len(ensemble.get_chain(thin=thin, discard=burnin)['model_0'][:, 0, 0, :, 0])\n",
    "\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(ndim, 1)\n",
    "fig.set_size_inches(10, 3*ndim)\n",
    "for i in range(ndim):     \n",
    "    for walk in range(20): # plot 20 walkers \n",
    "        ax[i].plot(ensemble.get_chain(thin=thin, discard=burnin)['model_0'][:, 0, walk, 0, i])\n",
    "        ax[i].hlines(truths[i], 0, len_chain, color='k', linestyle='-.', linewidth=0.8)\n",
    "    ax[i].set_ylabel(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corner plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin=400\n",
    "thin = 10\n",
    "\n",
    "samp = ensemble.get_chain(discard=burnin, thin=thin)['model_0'][:, 0, :, :, :].reshape(-1, ndim)\n",
    "print(\"sample shape:\", samp.shape)\n",
    "\n",
    "plt.figure()\n",
    "corner(\n",
    "    samp, bins=50, color=BLUE, \n",
    "    labels=labels, label_kwargs={'fontsize': 14}, \n",
    "    # range=lims,\n",
    "    truths=truths, truth_color=RED, \n",
    "    quantiles=[0.16, 0.5, 0.84],\n",
    "    show_titles=True, title_kwargs={'fontsize':14},\n",
    "    levels = (1. - np.exp(-1.**2/2), 1. - np.exp(-2.**2/2), 1. - np.exp(-3.**2/2)),\n",
    "    smooth=0.9, # default for bilby: smooth = 0.9, bins = 50 \n",
    "    plot_density=True, # whether to show the density of samples with colors \n",
    "    plot_datapoints=False, # whether to plot individual data points \n",
    "    fill_contours=True, # whether to fill the corners \n",
    "    );\n",
    "# plt.savefig(\"/home/ps/workspace/Triangle-BBH/Examples/Verification_corner.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The posterior remains to be refined with a heterodyned search ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triangle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
