{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the thread used by numpy \n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"  \n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import cupy as xp\n",
    "    # print(\"has cupy\")\n",
    "except (ImportError, ModuleNotFoundError) as e:\n",
    "    import numpy as xp\n",
    "    # print(\"no cupy\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "\n",
    "from Triangle.Constants import *\n",
    "from Triangle.Orbit import * \n",
    "from Triangle.Noise import *\n",
    "from Triangle.FFTTools import *\n",
    "from Triangle.TDI import XYZfromAET, AETfromXYZ\n",
    "\n",
    "from Triangle_BBH.Waveform import * \n",
    "from Triangle_BBH.Response import *\n",
    "from Triangle_BBH.Utils import *\n",
    "from Triangle_BBH.Fisher import *\n",
    "\n",
    "import multiprocessing\n",
    "# if __name__=='__main__':\n",
    "#     multiprocessing.set_start_method(\"fork\")\n",
    "    # multiprocessing.set_start_method(\"spawn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### orbit and noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orbit model \n",
    "orbit_file = '/home/ubuntu/TDCII/Triangle-Simulator/OrbitData/LISALikeOrbitEclipticTCB/20280322_LISA_3Mkm'\n",
    "orbit = Orbit(OrbitDir=orbit_file)\n",
    "\n",
    "# noise PSD model \n",
    "PSDfunc = TDIPSDs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### waveform and response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform \n",
    "mode = \"primary\"\n",
    "# mode = \"full\"\n",
    "if mode == \"primary\":\n",
    "    modes = [(2,2)]\n",
    "    approx = \"IMRPhenomD\"\n",
    "elif mode == \"full\": \n",
    "    modes = [(2,2), (2,1), (3,3), (3,2), (4,4), (4,3)]\n",
    "    approx = \"IMRPhenomHM\"\n",
    "else: \n",
    "    raise NotImplementedError(\"mode not implemented\")\n",
    "use_gpu = True \n",
    "WFG = BBHxWaveformGenerator(mode=mode, use_gpu=use_gpu)\n",
    "\n",
    "# tdi-2.0 response \n",
    "FDTDI = BBHxFDTDIResponseGenerator(orbit_class=orbit, waveform_generator=WFG, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data simulation (noiseless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings \n",
    "t_start = 60. * DAY \n",
    "Tobs = 3 * DAY \n",
    "t_end = t_start + Tobs \n",
    "fsample = 0.0667 \n",
    "delta_t = 1. / fsample\n",
    "delta_f = 1. / Tobs\n",
    "data_frequency = np.arange(int(fsample / 2. / delta_f)) * delta_f + delta_f\n",
    "print(\"frequency-domain data length:\", len(data_frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation spectral densities (CSD) at the median time \n",
    "arm_time = (t_end + t_start) / 2. \n",
    "arms = dict()\n",
    "for key in MOSA_labels:\n",
    "    arms[key] = orbit.LTTfunctions()[key](arm_time)\n",
    "print(\"arm lengths:\", arms)\n",
    "\n",
    "tmpxxs = PSDfunc.PSD_X2_unequal(data_frequency, arms)\n",
    "tmpyys = PSDfunc.PSD_Y2_unequal(data_frequency, arms)\n",
    "tmpzzs = PSDfunc.PSD_Z2_unequal(data_frequency, arms)\n",
    "tmpxys = PSDfunc.PSD_X2Y2star_unequal(data_frequency, arms)\n",
    "tmpyzs = PSDfunc.PSD_Y2Z2star_unequal(data_frequency, arms)\n",
    "tmpzxs = PSDfunc.PSD_Z2X2star_unequal(data_frequency, arms)\n",
    "Cov_XYZ = np.conjugate(np.array([\n",
    "    [tmpxxs, np.conjugate(tmpxys), tmpzxs],\n",
    "    [tmpxys, tmpyys, np.conjugate(tmpyzs)],\n",
    "    [np.conjugate(tmpzxs), tmpyzs, tmpzzs]\n",
    "]) / 4. / delta_f) # covariance matrix of shape (3, 3, Nf), defined as <channel_1 channel_2^*>\n",
    "InvCov_XYZ = np.linalg.inv(np.transpose(Cov_XYZ, (2, 0, 1))) # inverse covariance matrix of shape (Nf, 3, 3)\n",
    "print(\"noise CSD shape:\", Cov_XYZ.shape, InvCov_XYZ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set source parameters \n",
    "fiducial_parameters = {\n",
    "    'chirp_mass': 2e6, # [MSUN]\n",
    "    'mass_ratio': 0.4719138674211496, \n",
    "    'spin_1z': -0.219017,\n",
    "    'spin_2z': 0.777287,\n",
    "    'coalescence_time': 62., # [day]\n",
    "    'coalescence_phase': 3.123620213369193,\n",
    "    'luminosity_distance': 88470.94592236356, # [MPC]\n",
    "    'inclination': 0.6928647958196155, # [rad]\n",
    "    'longitude': 4.484587022801057, # [rad]\n",
    "    'latitude': 0.9023185939708002, # [rad]\n",
    "    'psi': 2.050979301490759 # [rad]\n",
    "    }\n",
    "\n",
    "# set response keywords \n",
    "channel_names = [\"X2\", \"Y2\", \"Z2\"]\n",
    "response_kwargs = dict(\n",
    "    modes=modes, \n",
    "    tmin=t_start / DAY,\n",
    "    tmax=t_end / DAY, \n",
    "    tc_at_constellation=False, \n",
    "    TDIGeneration=\"2nd\", \n",
    "    optimal_combination=False, \n",
    "    output_by_mode=False, \n",
    ")\n",
    "\n",
    "# calculate tdi response \n",
    "data_channels_FD_XYZ = FDTDI.Response(\n",
    "    parameters=fiducial_parameters, \n",
    "    freqs=xp.array(data_frequency),  \n",
    "    **response_kwargs, \n",
    ")\n",
    "\n",
    "# mask data at the null frequencies (around 0.25n Hz, n=1, 2, ... for Taiji)\n",
    "mask = np.ones_like(data_frequency, dtype=bool)\n",
    "for i in range(1, 80):\n",
    "    mask_idx = np.where(np.abs(data_frequency - 0.025 * i)<i*5e-4)[0] # empirical\n",
    "    mask[mask_idx] = False  \n",
    "print(\"number of dropped data points:\", np.where(mask==False)[0].shape)\n",
    "data_frequency = data_frequency[mask]\n",
    "data_channels_FD_XYZ = data_channels_FD_XYZ[:, mask]\n",
    "Cov_XYZ = Cov_XYZ[:, :, mask]\n",
    "InvCov_XYZ = InvCov_XYZ[mask]\n",
    "print(\"shapes of masked data\", data_frequency.shape, data_channels_FD_XYZ.shape, Cov_XYZ.shape, InvCov_XYZ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ich, nch in enumerate(channel_names):\n",
    "    plt.loglog(data_frequency, (np.abs(data_channels_FD_XYZ[ich].get()) * 2. * data_frequency), label=nch)\n",
    "    plt.loglog(data_frequency, (np.sqrt(data_frequency * 4. / Tobs * np.abs(Cov_XYZ[ich][ich]))), color=\"grey\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlim(5e-5, fsample/2)\n",
    "plt.ylim(1e-25, 1e-18)\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Characteristic strains\")\n",
    "plt.grid(linestyle=\":\")\n",
    "plt.title(\"TDI responses of \"+approx+\" waveform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Fisher information matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the parameters used in FIM and Likelihood \n",
    "parameter_names = [\n",
    "    'log_chirp_mass',\n",
    "    'mass_ratio',\n",
    "    'spin_1z',\n",
    "    'spin_2z',\n",
    "    'coalescence_time',\n",
    "    'coalescence_phase',\n",
    "    'log_luminosity_distance',\n",
    "    'cos_inclination',\n",
    "    'longitude',\n",
    "    'sin_latitude',\n",
    "    'psi'\n",
    "    ]\n",
    "\n",
    "fiducial_param_arr = ParamDict2ParamArr(fiducial_parameters)\n",
    "fiducial_param_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the waveform wrapper used by FIM \n",
    "def waveform_fisher_wrapper(param_dict, freqs):\n",
    "    param_arr = np.array(list(param_dict.values()))\n",
    "    p = ParamArr2ParamDict(param_arr)\n",
    "    test_channels_FD = FDTDI.Response(\n",
    "        parameters=p,\n",
    "        freqs=freqs,\n",
    "        **response_kwargs, \n",
    "    )\n",
    "    if use_gpu:\n",
    "        test_channels_FD = test_channels_FD.get() \n",
    "    return test_channels_FD   \n",
    "\n",
    "param_steps = dict()\n",
    "for k in parameter_names:\n",
    "    # param_steps[k] = -0.01\n",
    "    param_steps[k] = 1e-6\n",
    "    \n",
    "param_dict = dict()\n",
    "for i in range(len(parameter_names)):\n",
    "    param_dict[parameter_names[i]] = fiducial_param_arr[i]\n",
    "\n",
    "FIM = MultiChannelFisher(waveform_generator=waveform_fisher_wrapper, param_dict=param_dict, analyze_param_step_dict=param_steps, frequency=data_frequency, inverse_covariance=InvCov_XYZ, verbose=0)\n",
    "\n",
    "FIM.auto_test_step()\n",
    "FIM.calculate_Fisher()\n",
    "FIM.calculate_errors()\n",
    "fiducial_param_err = FIM.errors\n",
    "\n",
    "print(\"uncertainties of parameters:\")\n",
    "FIM.param_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Like = Likelihood(\n",
    "    response_generator=FDTDI, \n",
    "    frequency=data_frequency, \n",
    "    data=data_channels_FD_XYZ, \n",
    "    invserse_covariance_matrix=InvCov_XYZ, \n",
    "    response_parameters=response_kwargs, \n",
    "    use_gpu=use_gpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize heterodyned likelihood with base parameters or a base waveform \n",
    "# in practice, the base parameters should be obtained via MLE or other methods \n",
    "# Like.prepare_het_log_like(base_parameters=fiducial_param_arr.copy(), num_het_frequency=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test likelihood around the truths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) full likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test = 101\n",
    "param_idx = 0 # for example \n",
    "test_params = fiducial_param_arr[param_idx] + np.linspace(-1, 1, N_test) * fiducial_param_err[param_idx] * 3\n",
    "\n",
    "test_lls = np.zeros(N_test)\n",
    "for i_test in tqdm(range(N_test)):\n",
    "    tmp_params = fiducial_param_arr.copy()\n",
    "    tmp_params[param_idx] = test_params[i_test]\n",
    "    test_lls[i_test] = Like.full_log_like(tmp_params)\n",
    "plt.plot(test_params, test_lls)\n",
    "plt.title(parameter_names[param_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) vectorized full likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_params = np.zeros((11, N_test))\n",
    "for i_test in range(N_test):\n",
    "    tmp_params[:, i_test] = fiducial_param_arr.copy()\n",
    "    tmp_params[param_idx][i_test] = test_params[i_test]\n",
    "test_lls_vec = Like.full_log_like_vectorized(tmp_params)\n",
    "plt.plot(test_params, test_lls_vec)\n",
    "plt.title(parameter_names[param_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_params, test_lls, label=\"full likelihood\")\n",
    "plt.plot(test_params, test_lls_vec, linestyle=\":\", label=\"vectorized full likelihood\")\n",
    "plt.title(parameter_names[param_idx])\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run MCMC Sampling \n",
    "search with the full likelihood from a broad prior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eryn.ensemble import EnsembleSampler\n",
    "from eryn.state import State\n",
    "from eryn.prior import ProbDistContainer, uniform_dist\n",
    "from eryn.utils import TransformContainer\n",
    "from eryn.moves import GaussianMove, StretchMove, CombineMove\n",
    "from eryn.utils.utility import groups_from_inds\n",
    "from eryn.backends import HDFBackend\n",
    "from eryn.utils import SearchConvergeStopping\n",
    "\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set hyper parameters and priors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiducial_param_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper parameters\n",
    "ndim = 11 # dimension of paramters \n",
    "nwalkers = 200 # number of random walkers \n",
    "ntemps = 10 # number of temperatures used in parallel tempering \n",
    "temps = np.array(list(np.power(2., np.arange(ntemps - 1))) + [np.infty]) \n",
    "betas = 1. / temps \n",
    "tempering_kwargs=dict(betas=betas)\n",
    "\n",
    "mcmc_moves = StretchMove(a=2) # mcmc move \n",
    "# MHfactor = 2.38 ** 2 / ndim\n",
    "# MHcov = {'model_0': FIM.CovMatrix * MHfactor}\n",
    "# mcmc_moves = GaussianMove(MHcov)\n",
    "\n",
    "stop = None \n",
    "\n",
    "# set priors \n",
    "lim_lgMc = [5.5, 6.5]\n",
    "lim_q = [0.125, 0.999]\n",
    "lim_chiz1 = [-0.99, 0.99]\n",
    "lim_chiz2 = [-0.99, 0.99]\n",
    "lim_tc = [fiducial_param_arr[4]-500/DAY, fiducial_param_err[4]+500/DAY] \n",
    "lim_phic = [0, TWOPI]\n",
    "lim_lgD = [3.5, 5.5]\n",
    "lim_cosinc = [-1, 1]\n",
    "lim_lam = [0, TWOPI]\n",
    "lim_sinbeta = [-1, 1]\n",
    "lim_psi = [0, PI]\n",
    "\n",
    "lims = np.array([lim_lgMc, lim_q, lim_chiz1, lim_chiz2, lim_tc, lim_phic, lim_lgD, lim_cosinc, lim_lam, lim_sinbeta, lim_psi])\n",
    "lower_lims = lims[:, 0]\n",
    "upper_lims = lims[:, 1]\n",
    "\n",
    "priors_in = {i: uniform_dist(lims[i][0], lims[i][1]) for i in range(ndim)}\n",
    "priors = ProbDistContainer(priors_in)\n",
    "priors.use_cupy = False\n",
    "\n",
    "lims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eryn_like(params): \n",
    "    \"\"\"params: numpy array of shape (Nevents, Nparams)\"\"\"\n",
    "    return Like.full_log_like_vectorized(np.transpose(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "# pool = Pool(processes=multiprocessing.cpu_count())\n",
    "# pool = Pool(processes=24)\n",
    "# print(multiprocessing.cpu_count())\n",
    "\n",
    "init_ensemble = EnsembleSampler(\n",
    "    nwalkers,\n",
    "    ndim,\n",
    "    eryn_like, \n",
    "    priors,\n",
    "    args=[],\n",
    "    tempering_kwargs=tempering_kwargs,\n",
    "    stopping_fn=stop,\n",
    "    stopping_iterations=10,\n",
    "    moves=mcmc_moves,\n",
    "    vectorize=True,\n",
    "    # pool=pool,\n",
    ")\n",
    "\n",
    "filename='MCMCResults/FullLogLike.h5'\n",
    "backend = HDFBackend(filename)\n",
    "backend.reset(nwalkers=nwalkers, ndims=ndim, ntemps=ntemps, moves=init_ensemble.backend.move_keys)\n",
    "print('args of the backend =', backend.reset_args)\n",
    "print('move keys =', backend.move_keys)\n",
    "print('Initialization flag of backend =', backend.initialized)\n",
    "\n",
    "\n",
    "ensemble = EnsembleSampler(\n",
    "    nwalkers,\n",
    "    ndim,\n",
    "    eryn_like, \n",
    "    priors,\n",
    "    args=[],\n",
    "    tempering_kwargs=tempering_kwargs,\n",
    "    stopping_fn=stop,\n",
    "    stopping_iterations=10,\n",
    "    moves=init_ensemble.moves,\n",
    "    backend=backend,\n",
    "    vectorize=True,\n",
    "    # pool=pool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize starting positions throughout prior\n",
    "coords = priors.rvs(size=(ntemps, nwalkers,))\n",
    "print(coords.shape)\n",
    "thin_by = 100 \n",
    "burn = 0\n",
    "nsteps = int(100000 / thin_by)\n",
    "\n",
    "ensemble.stopping_fn = None\n",
    "\n",
    "out = ensemble.run_mcmc(coords, nsteps, burn=burn, progress=True, thin_by=thin_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume run after the run is stopped \n",
    "thin_by =100\n",
    "burn = 0\n",
    "nsteps = int(400000 / thin_by) # this should be far more than enough \n",
    "\n",
    "out = ensemble.run_mcmc(ensemble.get_last_sample(), nsteps, burn=burn, progress=True, thin_by=thin_by)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triangle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
